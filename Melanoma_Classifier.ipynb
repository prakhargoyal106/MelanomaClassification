{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!pip install efficientnet_pytorch torchtoolbox","execution_count":2,"outputs":[{"output_type":"stream","text":"Collecting efficientnet_pytorch\n  Downloading efficientnet_pytorch-0.6.3.tar.gz (16 kB)\nCollecting torchtoolbox\n  Downloading torchtoolbox-0.1.4.1-py3-none-any.whl (55 kB)\n\u001b[K     |████████████████████████████████| 55 kB 1.6 MB/s eta 0:00:011\n\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from efficientnet_pytorch) (1.5.1)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from torchtoolbox) (1.4.1)\nCollecting lmdb\n  Downloading lmdb-0.98.tar.gz (869 kB)\n\u001b[K     |████████████████████████████████| 869 kB 7.7 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from torchtoolbox) (4.45.0)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from torchtoolbox) (0.23.1)\nRequirement already satisfied: opencv-python in /opt/conda/lib/python3.7/site-packages (from torchtoolbox) (4.3.0.36)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from torchtoolbox) (1.14.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchtoolbox) (1.18.5)\nRequirement already satisfied: pyarrow in /opt/conda/lib/python3.7/site-packages (from torchtoolbox) (0.16.0)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet_pytorch) (0.18.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->torchtoolbox) (2.1.0)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->torchtoolbox) (0.14.1)\nBuilding wheels for collected packages: efficientnet-pytorch, lmdb\n  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.6.3-py3-none-any.whl size=12419 sha256=08086c28bbdaa02005f9c51b7a765e8fadddbc0ad154a923066782ff62311535\n  Stored in directory: /root/.cache/pip/wheels/90/6b/0c/f0ad36d00310e65390b0d4c9218ae6250ac579c92540c9097a\n  Building wheel for lmdb (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for lmdb: filename=lmdb-0.98-cp37-cp37m-linux_x86_64.whl size=273152 sha256=52cc657e79421a9a81f2ea4541fd7eb1ba7c5aaff2b2782899dfc3ff5e1e5b94\n  Stored in directory: /root/.cache/pip/wheels/9e/24/96/783d4dddcf63e3f8cc92db8b3af3c70cf6d76398bff77f1d5e\nSuccessfully built efficientnet-pytorch lmdb\nInstalling collected packages: efficientnet-pytorch, lmdb, torchtoolbox\nSuccessfully installed efficientnet-pytorch-0.6.3 lmdb-0.98 torchtoolbox-0.1.4.1\n","name":"stdout"}]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev","execution_count":2,"outputs":[{"output_type":"stream","text":"  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  5115  100  5115    0     0  25447      0 --:--:-- --:--:-- --:--:-- 25447\nUpdating... This may take around 2 minutes.\nUpdating TPU runtime to pytorch-nightly ...\nFound existing installation: torch 1.5.0\nUninstalling torch-1.5.0:\n  Successfully uninstalled torch-1.5.0\nFound existing installation: torchvision 0.6.0a0+35d732a\nUninstalling torchvision-0.6.0a0+35d732a:\nDone updating TPU runtime\n  Successfully uninstalled torchvision-0.6.0a0+35d732a\nCopying gs://tpu-pytorch/wheels/torch-nightly-cp37-cp37m-linux_x86_64.whl...\n- [1 files][108.5 MiB/108.5 MiB]                                                \nOperation completed over 1 objects/108.5 MiB.                                    \nCopying gs://tpu-pytorch/wheels/torch_xla-nightly-cp37-cp37m-linux_x86_64.whl...\n- [1 files][124.1 MiB/124.1 MiB]                                                \nOperation completed over 1 objects/124.1 MiB.                                    \nCopying gs://tpu-pytorch/wheels/torchvision-nightly-cp37-cp37m-linux_x86_64.whl...\n/ [1 files][  2.4 MiB/  2.4 MiB]                                                \nOperation completed over 1 objects/2.4 MiB.                                      \nProcessing ./torch-nightly-cp37-cp37m-linux_x86_64.whl\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch==nightly) (0.18.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch==nightly) (1.18.5)\n\u001b[31mERROR: fastai 1.0.61 requires torchvision, which is not installed.\u001b[0m\n\u001b[31mERROR: kornia 0.3.1 has requirement torch==1.5.0, but you'll have torch 1.7.0a0+fe41558 which is incompatible.\u001b[0m\n\u001b[31mERROR: allennlp 1.0.0 has requirement torch<1.6.0,>=1.5.0, but you'll have torch 1.7.0a0+fe41558 which is incompatible.\u001b[0m\nInstalling collected packages: torch\nSuccessfully installed torch-1.7.0a0+fe41558\nProcessing ./torch_xla-nightly-cp37-cp37m-linux_x86_64.whl\nInstalling collected packages: torch-xla\nSuccessfully installed torch-xla-1.6+9b55685\nProcessing ./torchvision-nightly-cp37-cp37m-linux_x86_64.whl\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from torchvision==nightly) (1.7.0a0+fe41558)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision==nightly) (1.18.5)\nRequirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from torchvision==nightly) (7.2.0)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch->torchvision==nightly) (0.18.2)\nInstalling collected packages: torchvision\nSuccessfully installed torchvision-0.8.0a0+1aef87d\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nThe following additional packages will be installed:\n  libgfortran4 libopenblas-base\nThe following NEW packages will be installed:\n  libgfortran4 libomp5 libopenblas-base libopenblas-dev\n0 upgraded, 4 newly installed, 0 to remove and 59 not upgraded.\nNeed to get 8550 kB of archives.\nAfter this operation, 97.6 MB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libgfortran4 amd64 7.5.0-3ubuntu1~18.04 [492 kB]\nGet:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libopenblas-base amd64 0.2.20+ds-4 [3964 kB]\nGet:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libopenblas-dev amd64 0.2.20+ds-4 [3860 kB]\nGet:4 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libomp5 amd64 5.0.1-1 [234 kB]\nFetched 8550 kB in 0s (38.3 MB/s) \ndebconf: delaying package configuration, since apt-utils is not installed\nSelecting previously unselected package libgfortran4:amd64.\n(Reading database ... 107461 files and directories currently installed.)\nPreparing to unpack .../libgfortran4_7.5.0-3ubuntu1~18.04_amd64.deb ...\nUnpacking libgfortran4:amd64 (7.5.0-3ubuntu1~18.04) ...\nSelecting previously unselected package libopenblas-base:amd64.\nPreparing to unpack .../libopenblas-base_0.2.20+ds-4_amd64.deb ...\nUnpacking libopenblas-base:amd64 (0.2.20+ds-4) ...\nSelecting previously unselected package libopenblas-dev:amd64.\nPreparing to unpack .../libopenblas-dev_0.2.20+ds-4_amd64.deb ...\nUnpacking libopenblas-dev:amd64 (0.2.20+ds-4) ...\nSelecting previously unselected package libomp5:amd64.\nPreparing to unpack .../libomp5_5.0.1-1_amd64.deb ...\nUnpacking libomp5:amd64 (5.0.1-1) ...\nSetting up libomp5:amd64 (5.0.1-1) ...\nSetting up libgfortran4:amd64 (7.5.0-3ubuntu1~18.04) ...\nSetting up libopenblas-base:amd64 (0.2.20+ds-4) ...\nupdate-alternatives: using /usr/lib/x86_64-linux-gnu/openblas/libblas.so.3 to provide /usr/lib/x86_64-linux-gnu/libblas.so.3 (libblas.so.3-x86_64-linux-gnu) in auto mode\nupdate-alternatives: using /usr/lib/x86_64-linux-gnu/openblas/liblapack.so.3 to provide /usr/lib/x86_64-linux-gnu/liblapack.so.3 (liblapack.so.3-x86_64-linux-gnu) in auto mode\nSetting up libopenblas-dev:amd64 (0.2.20+ds-4) ...\nupdate-alternatives: using /usr/lib/x86_64-linux-gnu/openblas/libblas.so to provide /usr/lib/x86_64-linux-gnu/libblas.so (libblas.so-x86_64-linux-gnu) in auto mode\nupdate-alternatives: using /usr/lib/x86_64-linux-gnu/openblas/liblapack.so to provide /usr/lib/x86_64-linux-gnu/liblapack.so (liblapack.so-x86_64-linux-gnu) in auto mode\nProcessing triggers for libc-bin (2.27-3ubuntu1) ...\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport torch\nimport torchvision\nimport torch.nn.functional as F\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau , CyclicLR\nfrom torchvision import transforms\n\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold\n\nfrom efficientnet_pytorch import EfficientNet\n#import torch_xla.core.xla_model as xm\n\nfrom joblib import Parallel, delayed\n\nimport os \nimport gc\nfrom tqdm.autonotebook import tqdm\n\nimport cv2\nimport datetime\nimport random\nimport warnings\nimport time\n\nfrom plotly.offline import iplot\nimport cufflinks\ncufflinks.go_offline()\ncufflinks.set_config_file(world_readable=True, theme='pearl')\n\nwarnings.simplefilter('ignore')\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)","execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/html":"        <script type=\"text/javascript\">\n        window.PlotlyConfig = {MathJaxConfig: 'local'};\n        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n        if (typeof require !== 'undefined') {\n        require.undef(\"plotly\");\n        requirejs.config({\n            paths: {\n                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n            }\n        });\n        require(['plotly'], function(Plotly) {\n            window._Plotly = Plotly;\n        });\n        }\n        </script>\n        "},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_csv = pd.read_csv('../input/jpeg-melanoma-384x384/train.csv')\ntest_csv = pd.read_csv('../input/jpeg-melanoma-384x384/test.csv')","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Removing all the data with NaN\ntrain_1 = train_csv.loc[pd.notnull(train_csv['sex'])]\ntrain_2 = train_1.loc[pd.notnull(train_csv['age_approx'])]\ntrain_3 = train_2.loc[pd.notnull(train_csv['anatom_site_general_challenge'])]","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"site = pd.get_dummies(train_3['anatom_site_general_challenge'], prefix = 'site')","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_3['sex'] = train_3['sex'].map({'male':1, 'female':0 })\ntest_csv['sex'] = test_csv['sex'].map({'male':1, 'female': 0})","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_3  = pd.concat([train_3 , site] , axis=1)","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"meta_features = ['sex', 'age_approx']  + [ f for f in site.columns]","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"site_test = pd.get_dummies(test_csv['anatom_site_general_challenge'], prefix= 'site')\ntest_csv = pd.concat([test_csv, site_test], axis=1)","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_csv.drop(['anatom_site_general_challenge'], axis=1, inplace = True)\ntrain_3.drop(['anatom_site_general_challenge'],axis=1, inplace=True)","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_3['sex'] = train_csv['sex']","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_3['sex'] = train_3['sex'].map({'male':1, 'female': 0})","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"meta_features = ['sex', 'age_approx'] + [c for c in train_3.columns if 'site' in c]","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class RemoveHair:\n  \"\"\"\n      Remove Hairs from images\n  \"\"\"\n  def __init__(self):\n    pass\n    \n  \n    \n  def __call__(self, image):\n\n    grayscale = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n\n    kernel = cv2.getStructuringElement(1, (17,17))\n\n    blackhat = cv2.morphologyEx(grayscale, cv2.MORPH_BLACKHAT, kernel)\n\n    _, threshold = cv2.threshold(blackhat, 10, 255, cv2.THRESH_BINARY)\n\n    final_image = cv2.inpaint(image, threshold, 1, cv2.INPAINT_TELEA)\n\n    return final_image\n\n","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Microscope:\n\n  def __init__(self, p):\n    \n    self.p = p\n\n  def __call__(self, img):\n\n\n    if random.random() < self.p:\n      circle = cv2.circle((np.ones(img.shape) * 255).astype(np.uint8),\n                          (img.shape[0]//2, img.shape[1]//2),\n                          (random.randint(img.shape[0]//2 , img.shape[1]//2)),\n                          (0,0,0),\n                          -1)\n      mask  = circle - 255\n      img = np.multiply(img,mask)\n    \n    return img\n\n","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Net(nn.Module):\n\n  def __init__(self, model, n_meta_features):\n    super().__init__()\n    \n    self.model = model\n    if 'EfficientNet' in str(model.__class__):\n      self.model._fc = nn.Linear(in_features=1280, out_features=500, bias=True)\n    \n    self.meta = nn.Sequential(nn.Linear(n_meta_features, 500),\n                              nn.BatchNorm1d(500),\n                              nn.ReLU(),\n                              nn.Dropout(p=0.2),\n                              nn.Linear(500,250),\n                              nn.BatchNorm1d(250),\n                              nn.ReLU(),\n                              nn.Dropout(p=0.2))\n    \n    self.output = nn.Linear(500 + 250, 1)\n\n  def forward(self, inputs):\n\n    x, meta = inputs\n    cnn_features = self.model(x)\n    meta_features = self.meta(meta)\n    features  = torch.cat((cnn_features, meta_features), dim = 1)\n    output = self.output(features)\n    \n    return output","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MelanomaDataset(Dataset):\n  \"\"\"\n    Our Dataset for Melanoma Classification\n    img_folder: Path to images directory\n    meta_features: Additional Data features to be used\n    df: Contains Meta_features\n    transforms: Data Augmentation Techniques to be applied\n  \"\"\"\n  def __init__(self, img_folder, df, meta_features = None, train = True, transforms = None ):\n    \n    super().__init__()\n\n    self.img_folder = img_folder\n    self.df = df\n    self.meta_features = meta_features\n    self.transforms = transforms\n    self.train = train\n    \n  \n  def __len__(self):\n    return len(self.df)\n\n  \n  def __getitem__(self, index):\n\n    img = os.path.join(self.img_folder , self.df.iloc[index]['image_name'] + '.jpg')\n    meta = np.array(self.df.iloc[index][meta_features].values, dtype= np.float32)\n    x = cv2.imread(img)\n\n    if self.transforms:\n      x = self.transforms(x)\n\n    if self.train:\n      y = self.df.iloc[index]['target']\n      return (x,meta) , y\n    else:\n      return (x,meta)\n\n\n\n","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_aug = transforms.Compose([RemoveHair(),\n                                Microscope(p=0.5),\n                                transforms.ToPILImage(),\n                                transforms.RandomHorizontalFlip(),\n                                transforms.RandomVerticalFlip(),\n                                transforms.ToTensor(),\n                                transforms.Normalize(mean=[0.485,0.456,0.406], std = [0.229, 0.224, 0.225])])\n\ntest_aug = transforms.Compose([RemoveHair(),\n                               transforms.ToPILImage(),\n                               transforms.ToTensor(),\n                               transforms.Normalize(mean=[0.485,0.456,0.406], std = [0.229, 0.224, 0.225])])\n\n\n                                \n                               \n\n","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"skf = GroupKFold(n_splits=5)\nmodel = EfficientNet.from_pretrained('efficientnet-b1')","execution_count":20,"outputs":[{"output_type":"stream","text":"Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b1-f1951068.pth\" to /root/.cache/torch/checkpoints/efficientnet-b1-f1951068.pth\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=31519111.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"276b56cd76184fe990c9a3120c57bcc4"}},"metadata":{}},{"output_type":"stream","text":"\nLoaded pretrained weights for efficientnet-b1\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 5\nmodel_path = 'model.pth'\n\noof = np.zeros((len(train_3), 1))\npreds = torch.zeros((len(test_csv), 1), dtype = torch.float32, device=device)\n","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = MelanomaDataset(img_folder = '../input/jpeg-melanoma-384x384/test',\n                      df = test_csv,\n                      meta_features = meta_features,\n                      train = False,\n                      transforms = test_aug)","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device","execution_count":22,"outputs":[{"output_type":"execute_result","execution_count":22,"data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"for fold, (train_idx, val_idx) in enumerate(skf.split(X = np.zeros(len(train_3)), \n                                                      y = train_3['target'], \n                                                      groups= train_3['patient_id'].tolist()), \n                                                      1):\n\n  print('='*20, 'Fold', fold, '='*20 )\n  model = EfficientNet.from_pretrained('efficientnet-b1')\n  best_val = 0\n  model = Net(model = model, n_meta_features = len(meta_features))\n\n  model = model.to(device)\n\n  optim = torch.optim.SGD(model.parameters(), lr=0.01)\n  scheduler = ReduceLROnPlateau(optimizer= optim, mode = 'max', patience = 1, verbose=True, factor=0.2)\n  criterion = nn.BCEWithLogitsLoss()\n  #print('dataset')\n  train = MelanomaDataset(df = train_3.iloc[train_idx].reset_index(drop=True),\n                          img_folder = '../input/jpeg-melanoma-384x384/train',\n                          meta_features = meta_features,\n                          train = True,\n                          transforms = train_aug)\n  \n  val = MelanomaDataset(df = train_3.iloc[val_idx].reset_index(drop=True),\n                        img_folder = '../input/jpeg-melanoma-384x384/train',\n                        meta_features = meta_features,\n                        train = True,\n                        transforms = test_aug)\n  \n  \n  \n  \n  #print('loader')\n  train_loader = DataLoader(dataset = train,\n                            batch_size = 32,\n                            shuffle = True,\n                            num_workers = 2)\n\n  \n  val_loader =  DataLoader(dataset = val,\n                            batch_size = 16,\n                            shuffle = False,\n                            num_workers = 2)\n  \n  \n  test_loader = DataLoader(dataset = test,\n                            batch_size = 16,\n                            shuffle = False,\n                            num_workers = 2)\n  \n  for epoch in range(epochs):\n\n    start_time = time.time()\n    correct = 0\n    epoch_loss = 0\n    model.train()\n    #print('0')\n    count=0\n    for x,y in train_loader:\n      \n      #if count==0:\n       #     print('yo')\n        #    count = 1\n      x[0] = torch.tensor(x[0], device = device, dtype= torch.float32)\n      x[1] = torch.tensor(x[1], device = device, dtype= torch.float32)\n      y    = torch.tensor( y, device = device, dtype= torch.float32)\n\n      optim.zero_grad()\n      z = model(x)\n\n      loss = criterion(z, y.unsqueeze(1))\n      loss.backward()\n      \n      optim.step()\n\n      pred = torch.round(torch.sigmoid(z))\n      correct += (pred.cpu() == y.cpu().unsqueeze(1)).sum().item()\n      epoch_loss += loss.item()\n    \n    train_acc = correct / len(train_idx)\n    model.eval()\n    val_preds = torch.zeros((len(val_idx), 1), dtype = torch.float32, device = device)\n\n    with torch.no_grad():\n\n      for j, (x_val, y_val) in enumerate(val_loader):\n\n        x_val[0] = torch.tensor(x_val[0], device = device, dtype= torch.float32)\n        x_val[1] = torch.tensor(x_val[1], device = device, dtype= torch.float32)\n        y_val    = torch.tensor(y_val   , device = device, dtype= torch.float32)\n        \n        z_val = model(x_val)\n        val_pred = torch.sigmoid(z_val)\n        val_preds[j*x_val[0].shape[0]:j*x_val[0].shape[0] + x_val[0].shape[0]] = val_pred\n      #print('2')\n      val_acc = accuracy_score( train_3.iloc[val_idx]['target'].values, torch.round(val_preds.cpu()))\n      val_roc = roc_auc_score( train_3.iloc[val_idx]['target'].values, val_preds.cpu())\n\n      print('Epoch {:03}: | Loss: {:.3f} | Train acc: {:.3f} | Val acc: {:.3f} | Val roc_auc: {:.3f} | Training time: {}'.format(\n            epoch + 1, \n            epoch_loss, \n            train_acc, \n            val_acc, \n            val_roc, \n            str(datetime.timedelta(seconds=time.time() - start_time))[:7]))\n\n      scheduler.step(val_roc)\n\n      if val_roc >= best_val:\n        best_val = val_roc\n        torch.save(model, model_path)\n  \n  model = torch.load(model_path)\n  model.eval()\n  val_preds = torch.zeros((len(val_idx), 1), dtype = torch.float32, device=device)\n  #print('3')\n  with torch.no_grad():\n\n    for j, (x_val, y_val) in enumerate(val_loader):\n      x_val[0] = torch.tensor(x_val[0], device = device, dtype= torch.float32)\n      x_val[1] = torch.tensor(x_val[1], device = device, dtype= torch.float32)\n      y_val    = torch.tensor(y_val   , device = device, dtype= torch.float32)\n\n      z_val = model(x_val)\n      val_pred = torch.sigmoid(z_val)\n      val_preds[j*x_val[0].shape[0]:j*x_val[0].shape[0] + x_val[0].shape[0]] = val_pred\n    oof[val_idx] = val_preds.cpu().numpy()\n    #print('4')\n    for i, x_test in enumerate(test_loader):\n      x_test[0] = torch.tensor(x_test[0], device = device, dtype = torch.float32)\n      x_test[1] = torch.tensor(x_test[1], device = device, dtype = torch.float32)\n      z_test    = model(x_test)\n      z_test    = torch.sigmoid(z_test)\n      z_test = torch.sigmoid(z_test)\n      preds[i*x_test[0].shape[0]:i*x_test[0].shape[0] + x_test[0].shape[0]] += z_test\n\n  del train, val, train_loader, val_loader, x, y, x_val, y_val\n  gc.collect\n\npreds /= skf.n_splits\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds1 = torch.zeros((len(test_csv), 1), dtype = torch.float32, device=device)","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"m = torch.load('../input/model-1/model.pth')","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nm.eval()\ntest_loader1 = DataLoader(dataset = test,\n                         batch_size = 16,\n                         shuffle = False,\n                         num_workers = 2)\nwith torch.no_grad():\n    for i, x_test in enumerate(test_loader1):\n\n          x_test[0] = torch.tensor(x_test[0], device = device, dtype = torch.float32)\n          x_test[1] = torch.tensor(x_test[1], device = device, dtype = torch.float32)\n          z_test    = m(x_test)\n          z_test    = torch.sigmoid(z_test)\n          z_test = torch.sigmoid(z_test)\n          preds1[i*x_test[0].shape[0]:i*x_test[0].shape[0] + x_test[0].shape[0]] += z_test\n      \n","execution_count":29,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat":4,"nbformat_minor":4}